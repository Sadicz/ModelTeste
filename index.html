<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VTuber Avatar com Rastreamento Facial e Voz</title>
    <style>
        body { margin: 0; overflow: hidden; }
        canvas { display: block; }
        #video {
            position: absolute;
            top: 10px;
            right: 10px;
            width: 320px;
            height: 240px;
            border: 1px solid #ccc;
        }
    </style>
</head>
<body>
    <video id="video" autoplay playsinline style="display: none;"></video>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r134/three.min.js"></script>
    <script src="https://unpkg.com/@pixiv/three-vrm@1.0.9/lib/three-vrm.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh@0.4/face_mesh.js"></script>
    <script>
        // Configuração da cena Three.js
        const scene = new THREE.Scene();
        const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        camera.position.set(0, 1.5, 1.5);
        camera.lookAt(0, 1.2, 0);

        const renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        document.body.appendChild(renderer.domElement);

        const light = new THREE.DirectionalLight(0xffffff, 0.8);
        light.position.set(1, 1, 1);
        scene.add(light);
        scene.add(new THREE.AmbientLight(0x404040));

        // Carregamento do modelo VRM
        let vrm = null;
        const loader = new THREE.VRMLoader();
        loader.load(
            'NullTapes.vrm', // Caminho relativo ao arquivo VRM na raiz
            (vrmModel) => {
                vrm = vrmModel;
                scene.add(vrm.scene);
                vrm.scene.rotation.y = Math.PI;
                console.log('Modelo VRM carregado com sucesso');
                initializeMedia();
            },
            (progress) => console.log('Carregando modelo:', progress.loaded / progress.total * 100, '%'),
            (error) => console.error('Erro ao carregar o modelo:', error)
        );

        // Configuração da Web Audio API e MediaPipe
        let audioContext, analyser, dataArray;
        const video = document.getElementById('video');

        async function initializeMedia() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                dataArray = new Uint8Array(analyser.frequencyBinCount);

                const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: true });
                const source = audioContext.createMediaStreamSource(stream);
                source.connect(analyser);
                video.srcObject = stream;
                video.play();
                setupFaceMesh();
            } catch (err) {
                console.error('Erro ao acessar microfone ou webcam:', err);
                alert('Não foi possível acessar o microfone ou a webcam. Verifique as permissões.');
            }
        }

        // Configuração do MediaPipe Face Mesh
        function setupFaceMesh() {
            const faceMesh = new FaceMesh({
                locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh@0.4/${file}`
            });
            faceMesh.setOptions({
                maxNumFaces: 1,
                refineLandmarks: true,
                minDetectionConfidence: 0.5,
                minTrackingConfidence: 0.5
            });
            faceMesh.onResults(onFaceMeshResults);
            const camera = new Camera(video, {
                onFrame: async () => {
                    await faceMesh.send({ image: video });
                },
                width: 640,
                height: 480
            });
            camera.start();
            animate();
        }

        // Processar resultados do Face Mesh
        function onFaceMeshResults(results) {
            if (results.multiFaceLandmarks && vrm && vrm.blendShapeProxy) {
                const landmarks = results.multiFaceLandmarks[0];
                if (landmarks) {
                    const mouthTop = landmarks[13];
                    const mouthBottom = landmarks[14];
                    const mouthOpen = Math.abs(mouthTop.y - mouthBottom.y) * 10;
                    vrm.blendShapeProxy.setValue('A', Math.min(mouthOpen, 1));

                    const leftEyeTop = landmarks[159];
                    const leftEyeBottom = landmarks[145];
                    const rightEyeTop = landmarks[386];
                    const rightEyeBottom = landmarks[374];
                    const leftBlink = Math.abs(leftEyeTop.y - leftEyeBottom.y) * 20;
                    const rightBlink = Math.abs(rightEyeTop.y - rightEyeBottom.y) * 20;
                    vrm.blendShapeProxy.setValue('blink_l', Math.min(leftBlink, 1));
                    vrm.blendShapeProxy.setValue('blink_r', Math.min(rightBlink, 1));
                }
            }
        }

        // Função de animação
        function animate() {
            requestAnimationFrame(animate);

            if (vrm && analyser) {
                analyser.getByteFrequencyData(dataArray);
                const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
                const mouthOpenValue = Math.min(average / 255, 1);
                if (!vrm.blendShapeProxy.getValue('A')) {
                    vrm.blendShapeProxy.setValue('A', mouthOpenValue);
                }
                vrm.update();
            }

            renderer.render(scene, camera);
        }

        // Ajustar tamanho da tela
        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });
    </script>
</body>
</html>
